# LesFaits

This repository contains all the tools and code used to build the LesFaits dataset to train the GPT-Vérité models as well as the code to train the tokenizer.

This repository is fork from [BigScience Roots Corups](https://github.com/bigscience-workshop/data-preparation)


## Key resources
### [Sourcing the subdataset](sourcing)

### [Cleaning](preprocessing/training/01b_cleaning)

### [More Cleaning](preprocessing/training/01a_cleaning)

### [Code used for the tokenizer's training, aggregation and creation of subdataset](preprocessing/tokenizer)

### [Code for the Entity Tokenizer](entity_tokenizer)

