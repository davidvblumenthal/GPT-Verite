---
abstract: 'Melanoma is one of ten most common cancers in the US. Early detection is crucial for survival, but often the cancer is diagnosed in the fatal stage. Deep learning has the potential to improve cancer detection rates, but its applicability to melanoma detection is compromised by the limitations of the available skin lesion data bases, which are small, heavily imbalanced, and contain images with occlusions. We build deep-learning-based tools for data purification and augmentation to counter-act these limitations. The developed tools can be utilized in a deep learning system for lesion classification and we show how to build such system. The system heavily relies on the processing unit for removing image occlusions and the data generation unit, based on generative adversarial networks, for populating scarce lesion classes, or equivalently creating virtual patients with pre-defined types of lesions. We empirically verify our approach and show that incorporating these two units into melanoma detection system results in the superior performance over common baselines.'
author:
- |
    Devansh Bisla, Anna Choromanska\
    Dept of Electrical Engineering,\
    Tandon School of Engineering, New York University\
    [bisla@nyu.edu, ac5455@nyu.edu]{}
- |
    Russell S. Berman\
    Division of Surgical Oncology, Department of Surgery,\
    New York University School of Medicine\
- |
    Jennifer A. Stein, David Polsky\
    Ronald O. Perelman Department of Dermatology,\
    New York University School of Medicine
bibliography:
- 'ms.bib'
title: |
    Towards Automated Melanoma Detection with Deep Learning:\
    Data Purification and Augmentation
---

Introduction
============

According to the American Cancer Society, melanoma takes one life in every $54$ minutes [@CancerFa15:online] in US. In Australia, where the incidence of melanoma are the highest in the world along with New Zealand, one person dies every five hours from melanoma [@CancerAu15:online]. There were approximately $87,110$ cases of melanoma in the US and $13,941$ in Australia [@AustraliaGov] alone in the year $2017$. Even though it accounts for less than $1\%$ of the total skin diseases, it is the major cause of deaths related to these diseases. The $5$-year survival rate in the US is $98\%$ and reduces down to $18\%$ once it spreads to distant organs. Therefore, early detection of melanoma is of fundamental importance to increase the survival rates. The techniques and technologies that aim at automating the visual examination of skin lesions, traditionally done by dermatologists, are targeted to i) assist clinicians with navigating patients lesions and detecting early signs of cancer, ii) enable every patient to assess their lesions, iii) promote melanoma prevention and increase the awareness of this disease, and iv) provide the platform for massive data collection stimulating further research on skin cancer. The fundamental obstacle in advancing automated methods is the lack of large and balanced data sets that can be used to train computational models, i.e. many publicly available skin lesion data sets are small, imbalanced (contain significant disproportions in the number of data points between different classes of lesions and are heavily dominated by the images of benign lesions), and furthermore contain occlusions such as hairs. Publicly available data-sets are obtained from multiple different imaging centers, hospitals, and research institutes, each with different data collection and management standards. Furthermore, some imaging centers mark lesions for example by placing the ruler next to the lesion to measure the diameter of the lesion. Such practices skew the data to comply with the requirements of a particular organization but also introduce bias to the data. We contribute to the computer-aided dermatological techniques with a new set of tools for careful preparation of the training data that mitigate the above mentioned negative data aspects compromising deep network performance in practice. The data preparation consists of the purification and augmentation stages. The process of data purification, removal of occlusions such as hairs and rulers from the images, relies on the efficient lesion segmentation method combined with traditional data processing techniques. The process of data augmentation is two-folded and comprises of balancing the data through the generation of artificial dermoscopic images and performing additional data augmentation. Our tools for data purification and augmentation eventually enhance the performance of deep-learning-based systems for lesion classification. The block diagram of the complete system is captured in Figure \[fig:block\].

![The block diagram of the complete system for lesion classification. Lesion segmentation, data purification, and data generation models are utilized to purify and augment the training data set.[]{data-label="fig:block"}](Results/block.png){width="45.00000%"}

We tested our approach[^1] on popular ISIC-2017 data set [@ISIC] containing lesion images from three classes: melanoma, nevus, and seborrheic keratosis. Additional results on the ISIC 2018 [@Tschandl2018_HAM10000] data set containing lesion images from seven classes: acitinic keratosis, basal cell carcinoma, benign keratosis, dermatofibroma, melanoma, nevus, and vascular lesion are provided in the Supplementary section. The paper is organized as follows: Section \[sec:R\] describes related work, Section \[sec:data\] discusses the data, Section \[sec:DP\] explains our approach for data purification that relies on the segmentation step, Section \[sec:DG\] addresses the problem of data imbalancedness with a tool based on generative adversarial network for synthetic data generation, and finally Section \[sec:E\] incorporates the proposed solutions into a classification network and reports experimental results. Section \[sec:CC\] concludes the paper. Supplement contains additional results.

Related Work {#sec:R}
============

There is a substantial body of work on developing algorithms for computer-assisted dermatology. A number of approaches rely on hand-crafted features [@Barata; @DBLP:conf/dicta/YaoWXGF16; @DBLP:conf/isbi/BiKAFF16; @Yan] and are not scalable to massive data sets. Recent development of the field of deep learning has triggered a sudden shift from vision algorithms largely relying on expensive manual feature extraction. The development of convolutional neural networks (CNNs) [@lecun], and in particular AlexNet [@alexnet], is widely regarded as the turning point in deep learning research. CNN-based approaches later outsmarted humans in several tasks ranging from image classification to playing GO [@go]. Consequently, the techniques for automatic analysis of skin lesions recently became dominated with deep learning algorithms. A striking majority of these approaches are ensemble techniques. They include i) ensembles of different architectures used for extracting data features, which are further inputted to the SVM classifier [@Codella2016DeepLE; @DBLP:journals/corr/MenegolaTFLAV17; @DBLP:conf/isbi/MenegolaFPBAV17] (deep learning combined with SVM is also used in other approaches [@conf/miccai/CodellaCAGHS15]), ii) segmentation networks followed by bagging-type ensemble strategies [@DBLP:journals/corr/YuanCL17], iii) segmentation networks that use smooth F1-score loss function to factor-in data imbalancedness [@Kawahara2017FullyCN], iv) methods that interpolate between the outputs obtained from two fully-convolutional residual networks and refine the results with lesion index calculation unit [@DBLP:journals/corr/LiS17a], v) highly-complicated ensembles with multi-view image generator [@DBLP:journals/corr/MatsunagaHMK17], vi) systems based on multiple segmentation networks that provide segmentations conditioned on the pre-defined sets of local patterns that are of interest to clinicians in their diagnosis [@DBLP:journals/corr/Gonzalez-Diaz17], and finally vii) ensemble techniques in which multiple fully convolutional networks learn complementary visual characteristics of different skin lesions as well as lesion boundaries and an integration method forms a final segmentation from individual segmentations [@Bi2017DermoscopicIS]. Non-ensemble strategies propose using i) U-Net model that relies on CNN with skipped connections [@doi:10.1080/24699322.2017.1389405] for lesion segmentation and also classification and ii) residual network ResNet-50 pre-trained on ImageNet [@Cicero] for lesion classification, or iii) GoogleNet Inception v3 CNN architecture pre-trained on ImageNet for lesion classification [@Esteva2017], where in the latter paper the experiments are done on a data base that is not publicly available. Only a subset of these deep learning methods perform data augmentation to increase the training data size [@Increasing] or balance the data, but they use fairly standard tools, i.e. image cropping, scaling, and flipping, and none of them addresses the problem of the removal of image occlusions (note that throughout the paper we refer to cropping, scaling and flipping as the standard/traditional augmentation techniques).

Proposed Methods
================

Data sets {#sec:data}
---------

We focus on dermoscopic images which allow higher diagnostic accuracy than traditional eye examination [@dermo] as they show the micro-structures in the lesion and provide a view unobstructed from skin reflections. We took $803$ cases of melanoma, $2107$ cases of nevus, and $288$ cases of seborrheic keratosis from the ISIC 2017 challenge dataset, $40$ cases of melanoma and $80$ cases of nevus from the PH$^2$ data set[@PH2], and $76$ cases of melanoma, $331$ cases of nevus and $257$ cases of seborrheic keratosis from the Edinburgh data set [@Dermofit] (see Figure \[fig:il\]). Diagnoses of these lesions were established clinically (ISIC, Dermofit) and/or via histopathology (ISIC, PH$^{2}$) , which will be discussed more in details later in the paper, regarding the training data size). Additionally, each data set contains lesion segmentation masks. We tested our models on ISIC 2017 and ISIC 2018 (Supplementary section) test data sets obtained from the ISIC Archives.

Data Purification {#sec:DP}
-----------------

### Data Purification Problem

![Visualization results for the conventionally-trained model on the ISIC 2017 data set. **(Top)**: Original image. **(Bottom)**: Visualization mask overlaid on the original image. The model overfits to image occlusions such as hairs and rulers. []{data-label="fig:visualization"}](Results/ISIC2018/Classification/Visualization/ISIC_0014221.jpg "fig:"){width="9.00000%"} ![Visualization results for the conventionally-trained model on the ISIC 2017 data set. **(Top)**: Original image. **(Bottom)**: Visualization mask overlaid on the original image. The model overfits to image occlusions such as hairs and rulers. []{data-label="fig:visualization"}](Results/ISIC2018/Classification/Visualization/ISIC_0013472.jpg "fig:"){width="9.00000%"} ![Visualization results for the conventionally-trained model on the ISIC 2017 data set. **(Top)**: Original image. **(Bottom)**: Visualization mask overlaid on the original image. The model overfits to image occlusions such as hairs and rulers. []{data-label="fig:visualization"}](Results/ISIC2018/Classification/Visualization/ISIC_0013733.jpg "fig:"){width="9.00000%"} ![Visualization results for the conventionally-trained model on the ISIC 2017 data set. **(Top)**: Original image. **(Bottom)**: Visualization mask overlaid on the original image. The model overfits to image occlusions such as hairs and rulers. []{data-label="fig:visualization"}](Results/ISIC2018/Classification/Visualization/ISIC_0013767.jpg "fig:"){width="9.00000%"} ![Visualization results for the conventionally-trained model on the ISIC 2017 data set. **(Top)**: Original image. **(Bottom)**: Visualization mask overlaid on the original image. The model overfits to image occlusions such as hairs and rulers. []{data-label="fig:visualization"}](Results/ISIC2018/Classification/Visualization/ISIC_0014876.jpg "fig:"){width="9.00000%"} ![Visualization results for the conventionally-trained model on the ISIC 2017 data set. **(Top)**: Original image. **(Bottom)**: Visualization mask overlaid on the original image. The model overfits to image occlusions such as hairs and rulers. []{data-label="fig:visualization"}](Results/ISIC2018/Classification/Visualization/ISIC_0014221overlayedmap.jpg "fig:"){width="9.00000%"} ![Visualization results for the conventionally-trained model on the ISIC 2017 data set. **(Top)**: Original image. **(Bottom)**: Visualization mask overlaid on the original image. The model overfits to image occlusions such as hairs and rulers. []{data-label="fig:visualization"}](Results/ISIC2018/Classification/Visualization/ISIC_0013472overlayedmap.jpg "fig:"){width="9.00000%"} ![Visualization results for the conventionally-trained model on the ISIC 2017 data set. **(Top)**: Original image. **(Bottom)**: Visualization mask overlaid on the original image. The model overfits to image occlusions such as hairs and rulers. []{data-label="fig:visualization"}](Results/ISIC2018/Classification/Visualization/ISIC_0013733overlayedmap.jpg "fig:"){width="9.00000%"} ![Visualization results for the conventionally-trained model on the ISIC 2017 data set. **(Top)**: Original image. **(Bottom)**: Visualization mask overlaid on the original image. The model overfits to image occlusions such as hairs and rulers. []{data-label="fig:visualization"}](Results/ISIC2018/Classification/Visualization/ISIC_0013767overlayedmap.jpg "fig:"){width="9.00000%"} ![Visualization results for the conventionally-trained model on the ISIC 2017 data set. **(Top)**: Original image. **(Bottom)**: Visualization mask overlaid on the original image. The model overfits to image occlusions such as hairs and rulers. []{data-label="fig:visualization"}](Results/ISIC2018/Classification/Visualization/ISIC_0014876overlayedmap.jpg "fig:"){width="9.00000%"}

Dermoscopic images often contain occlusions such as hairs and/or rulers. Deep learning approaches in general can handle such image occlusions and learn to avoid these objects while making predictions, though their learning ability is conditioned upon the availability of large training data. In case of dermoscopic data bases, which typically have small or medium size, the deep learning models are easily prone to overfitting, i.e. they use visual cues such as hairs and rulers as indicators of the lesion category. To demonstrate this problem we employ the visualization technique, called VisualBackProp [@visualization], that highlights the part of the image that the network focuses on when forming its prediction. Figure \[fig:visualization\] shows the results obtained for the traditionally-trained deep model (without performing data purification or augmentation) on the raw data.

### Data Purification Method

We utilize traditional data processing approaches to find and remove hairs and rulers on the images. We extended the hair-removal algorithm [@hair_removal]. One of its steps involves thresholding the luminance channel of the image in the LUV color space, which may also remove dark regions belonging to the lesion itself. To correct that, we overlay the processed image with the segmented lesion obtained from our segmentation network that will be described later. The holes in the background were eliminated by performing the closing operation. We compare two different input images before and after pre-processing to judge the quality of the purification algorithm. This is captured in Figure \[hair\_removal\]. We preprocessed $470$ cases of nevus, $147$ cases of melanoma, and $182$ cases of seborrheic keratosis, which contained hairs and rulers and removed those from the images. The pre-processed images were added to the training data set of the lesion classification model to make it more robust to the presence of occlusions and prevent over-fitting.

     Layer        Output size                 Kernel       Stride   Padding   Dilation
  -- ------------ --------------------------- ------------ -------- --------- ----------
     Conv         64$\times$378$\times$378    3$\times$3   1        0         1
     Conv         64$\times$376$\times$376    3$\times$3   1        0         1
     Maxpool      64$\times$188$\times$188    2$\times$2   2        0         1
     Conv         128$\times$186$\times$186   3$\times$3   1        0         1
     Conv         128$\times$184$\times$184   3$\times$3   1        0         1
     Maxpool      128$\times$92$\times$92     2$\times$2   2        0         1
     Conv         256$\times$90$\times$90     3$\times$3   1        0         1
     Conv         256$\times$88$\times$88     3$\times$3   1        0         1
     Maxpool      256$\times$44$\times$44     2$\times$2   2        0         1
     Conv         512$\times$40$\times$40     3$\times$3   1        0         2
     Conv         512$\times$36$\times$36     3$\times$3   1        0         2
     Conv         1024$\times$28$\times$28    3$\times$3   1        0         4
     Conv         1024$\times$20$\times$20    3$\times$3   1        0         4
     Trans-Conv   1024$\times$28$\times$28    3$\times$3   1        0         4
     Trans-Conv   512$\times$36$\times$36     3$\times$3   1        0         4
     Trans-Conv   512$\times$40$\times$40     3$\times$3   1        0         2
     Trans-Conv   512$\times$44$\times$44     3$\times$3   1        0         2
     Up-Conv      256$\times$88$\times$88     3$\times$3   1        1         1
     Trans-Conv   256$\times$90$\times$90     3$\times$3   1        0         1
     Trans-Conv   256$\times$92$\times$92     3$\times$3   1        0         1
     Up-Conv      128$\times$184$\times$184   3$\times$3   1        1         1
     Trans-Conv   128$\times$186$\times$186   3$\times$3   1        0         1
     Trans-Conv   128$\times$188$\times$188   3$\times$3   1        0         1
     Up-Conv      64$\times$376$\times$376    3$\times$3   1        1         1
     Trans-Conv   64$\times$378$\times$378    3$\times$3   1        0         1
     Trans-Conv   64$\times$380$\times$380    3$\times$3   1        0         1
     Conv         1$\times$380$\times$380     1$\times$1   1        0         1

### Segmentation Network {#sec:SN}

(A)

![**(A)** Architecture of the Segmentation Network. Black lines represent skipped connections. Each convolution operation is followed by ReLU activation; 2 dil and 4 dil refers to resp. 2 and 4 dilated convolution operations. **(B)** **Top**: Original images from the test data. **Middle**: Segmentation masks overlaid on the original images. **Bottom**: Corresponding segmentation masks obtained from Segmentation Network overlaid on the original images.[]{data-label="fig:seg"}](Results/ISIC2018/Segmentation/cnn.png){width="\textwidth"}

\
(B)

For the lesion segmentation, we utilized a U-Net architecture [@unet], which was previously successfully applied to the problem of medical image segmentation and won the ISBI Cell Tracking Challenge [@ISBI] in 2015. We modified the U-Net architecture, since it contains max pooling layers, which cause the increase of the field of view of convolutional filters as the input propagates through the model but at the same time also reduces the resolution of the input images. We however require output and input of the segmentation network to have the same dimension. To overcome this problem, we added dilated [@dilated] convolutions in the final layers of the down-sampling block of our architecture. We furthermore removed the up-sampling blocks in the traditional U-Net architecture and instead added transposed convolutional layers. The architecture of the model is presented in Figure \[fig:seg\] and Table \[tab:U-Net\]. The input to the segmentation architecture consists of seven channels (RGB and HSV channels and luminance). Thus the network input image size is $7\times 380\times 380$. We normalized each channel of the input to mean value $0.5$ and standard deviation $0.5$. We used a binary cross-entropy loss to train the network and an Adam [@adam] optimizer with a global learning rate of $1e^{-4}$ and beta values of $(0.9,0.999)$. These settings of optimization hyper-parameters led to the best performance. Furthermore, the output of the segmentation model was passed through a binary hole filling procedure to fill empty holes in the segmentation mask. The segmentation masks obtained for test lesions are demonstrated in Figure \[fig:seg\]. To evaluate the performance of the segmentation model the ISIC challenge proposes to use the Jaccard index. The Jaccard index, also known as the intersection over union is used to measure the similarity between two sample sets and is defined as the size of the intersection over the size of the union of the two sets. To calculate the index for segmentation masks we evaluate the area of the overlap between the true label and model output and divide it by the area of the union between them. We obtained an average index of $0.77$ while the best performer of the ISIC 2017 challenge obtained $0.765$.

Data Augmentation {#sec:DG}
-----------------

### Data Imbalancedness Problem

The data imbalancedness, illustrated in Figure \[fig:il\], is yet another significant factor that deteriorates the performance of deep learning systems analyzing dermoscopic images. The classifiers tend to be biased towards majority classes that correspond to benign lesions. This problem can be partially mitigated by introducing higher penalty for misclassifying rare classes, though data augmentation techniques replace this approach nowadays as they have the advantage of increasing data variability while balancing the data. We propose to use data augmentation technique that relies on generating the images from scarce classes that obey the data distribution of these classes. This is equivalent to creating virtual patients with lesions from scarce classes in order to even their size with the large-size classes.

### Data Generation Network with de-coupled DCGANs

(A)

![**(A)** Architecture of the DCGAN model. **(B)** Histograms of the MSE values for **(left)** melanoma (the mean and std of the MSE are $0.088 \pm  0.052$) **(right)** seborrheic keratosis (the mean and std of the MSE are $0.030 \pm 0.015$).[]{data-label="fig:histo"}](Results/ISIC2017/Generation/DCGAN.png){width="\textwidth"}

\
(B)

![**(A)** Architecture of the DCGAN model. **(B)** Histograms of the MSE values for **(left)** melanoma (the mean and std of the MSE are $0.088 \pm  0.052$) **(right)** seborrheic keratosis (the mean and std of the MSE are $0.030 \pm 0.015$).[]{data-label="fig:histo"}](Results/ISIC2017/Generation/M/Histogram/M_Histogram-eps-converted-to.pdf "fig:"){width="49.00000%"} ![**(A)** Architecture of the DCGAN model. **(B)** Histograms of the MSE values for **(left)** melanoma (the mean and std of the MSE are $0.088 \pm  0.052$) **(right)** seborrheic keratosis (the mean and std of the MSE are $0.030 \pm 0.015$).[]{data-label="fig:histo"}](Results/ISIC2017/Generation/SK/Histogram/SK_Histogram-eps-converted-to.pdf "fig:"){width="49.00000%"}

\

(A)

![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/Histogram/image182_mel-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/Histogram/image19_mel-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/Histogram/image9_mel-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/Histogram/image439_mel-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/Histogram/image160_mel-eps-converted-to.pdf "fig:"){width="19.00000%"}

\
(B)

![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/Histogram/image19_sk-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/Histogram/image29_sk-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/Histogram/image4_sk-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/Histogram/image65_sk-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/Histogram/image222_sk-eps-converted-to.pdf "fig:"){width="19.00000%"}

\
(C)

![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/around_lowest/image89_mel-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/around_lowest/image113_mel-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/around_lowest/image182_mel-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/around_lowest/image118_mel-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/around_lowest/image156_mel-eps-converted-to.pdf "fig:"){width="19.00000%"}

![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/around_lowest/image89_m-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/around_lowest/image113_m-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/around_lowest/image182_m-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/M/around_lowest/image118_m-eps-converted-to.pdf "fig:"){width="19.00000%"}

\
(D)

![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/around_lowest/image437_sk-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/around_lowest/image19_sk-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/around_lowest/image78_sk-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/around_lowest/image196_sk-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/around_lowest/image39_sk-eps-converted-to.pdf "fig:"){width="19.00000%"}

![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/around_lowest/image437_m-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/around_lowest/image19_m-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/around_lowest/image78_m-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/around_lowest/image196_m-eps-converted-to.pdf "fig:"){width="19.00000%"} ![ **(A)** Generated melanoma images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(B)** generated seborrheic keratosis images **(top)** and the original images from the training set **(bottom)** for different values of MSE. **(C)** generated melanoma images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE. **(D)** generated seborrheic keratosis images **(top)** and the original images from the training data set **(bottom)** for the smallest obtained values of MSE.[]{data-label="fig:examp"}](Results/ISIC2017/Generation/SK/around_lowest/image39_m-eps-converted-to.pdf "fig:"){width="19.00000%"}\

     Layer       Output Size                Kernel       Stride   Padding   
  -- ----------- -------------------------- ------------ -------- --------- --
     TransConv   256$\times$4$\times$4      4$\times$4   1        0         
     TransConv   128$\times$8$\times$8      4$\times$4   2        1         
     TransConv   64$\times$16$\times$16     4$\times$4   2        1         
     TransConv   32$\times$32$\times$32     4$\times$4   2        1         
     TransConv   16$\times$64$\times$64     4$\times$4   2        1         
     TransConv   8$\times$128$\times$128    4$\times$4   2        1         
     TransConv   3$\times$256$\times$256    4$\times$4   2        1         
     Conv        16$\times$128$\times$128   4$\times$4   2        1         
     Conv        32$\times$64$\times$64     4$\times$4   2        1         
     Conv        64$\times$32$\times$32     4$\times$4   2        1         
     Conv        128$\times$16$\times$16    4$\times$4   2        1         
     Conv        256$\times$8$\times$8      4$\times$4   2        1         
     Conv        512$\times$4$\times$4      4$\times$4   1        1         
     Conv        1$\times$1$\times$1        4$\times$4   1        0         

We propose the data generation method utilizing de-coupled DCGANs. We use two separate Deep Convolutional Generative Adversarial Networks (DCGANs) [@DCGAN] to generate $350$ images of melanoma and $750$ images of seborrheic keratosis, which were the two classes heavily under-represented in the ISIC 2017 data set compared to a much larger nevus class. Since we use separate networks for each class, we refer to this approach as “de-coupled DCGANs”. ISIC-2018 dataset contains large number of classes thus, we couple the DCGANs [@DBLP:journals/corr/0001T16] making the initial layers share parameters. This technique is highlighted in the supplementary section of the paper. We extended the architecture of DCGAN to produce images of resolution $256 \times 256$. The model architecture is highlighted in the Figure \[fig:histo\] and Table \[tab:DCGAN\]. GAN techniques rely on training a generator network to generate images which have similar distribution to the one followed by the training data. The discriminator provides a feedback how close the two distributions are. In our experiments, the latent vector of length 10 that inputs the generator is obtained from standard Gaussian distribution with mean 0 and standard deviation 1. We modify DCGAN to enable the generation of images with the desired resolution by adding layers to both generator and discriminator. Binary cross entropy loss and Adam optimizer with learning rate of $2e^{-4}$ and beta values of $0.5$ and $0.999$ were used to train both discriminator and generator. To prevent generator from collapsing and perform stable discriminator-generator optimization we utilize stabilization techniques [@train_gan] and perform early stopping while training the network. Furthermore, we perform two rounds of additional generator training after every $10^{\text{th}}$ round of joint training of both discriminator and generator. The process of data generation needs to be done carefully. It is essential to make sure that the generated images differ from the ones contained in the training data to maximize data variety. In order to verify that, we calculate the mean squared error (MSE) between each generated image to all the images from the training data set and choose the training image that corresponds to the minimal value of the MSE. We then compared each generated image with its closest, i.e. MSE-minimizing, training image to make sure they are not duplicates. Figure \[fig:histo\] shows the histograms of the mean squared error (MSE) for seborrheic keratosis and melanoma. The histograms indicate the wide variation in the images generated by the model. Figure \[fig:examp\] highlights some exemplary images from both melanoma and seborrheic keratosis classes for different values of MSE. It also presents additional images with smallest obtained MSE i.e images obtained from the extreme left side of both histograms. We finally augment the data in the two considered classes by performing horizontal flipping of images such that the class sizes increase to $2685$ for melanoma and $2772$ for seborrheic keratosis. The process of augmenting the training data by adding purified images, GAN-based data generation, and mentioned traditional augmentation is captured in Figure \[fig:il\]. We finally augment the entire data set using vertical flipping and random cropping to increase the data set further $6$ times. The final training data set was obtained and that will be used for the classification model is balanced and contains $16110$ melanoma cases, $17928$ nevus cases, and $16632$ seborrheic keratosis cases, among which a notable fraction, i.e $26\%$, constitute the artificially-generated data.

![Illustration of the process of gradually increasing the size of the training data for the classification model through data pre-processing (hairs and rulers removal), data generation with DCGAN, and data augmentation (Data-Aug) using standard techniques.[]{data-label="fig:il"}](Results/ISIC2017/data.png){width="45.00000%"}

Experiments {#sec:E}
===========

Classification Network
----------------------

Nevus Melanoma Seborrheic Keratosis\
![image](Results/ISIC2017/Visualization/N/TP/ISIC_0012149-eps-converted-to.pdf){width="7.50000%"} ![image](Results/ISIC2017/Visualization/N/FP/ISIC_0012207-eps-converted-to.pdf){width="7.50000%"} ![image](Results/ISIC2017/Visualization/N/FN/ISIC_0016009-eps-converted-to.pdf){width="7.50000%"} ![image](Results/ISIC2017/Visualization/N/TN/ISIC_0012758-eps-converted-to.pdf){width="7.50000%"} ![image](Results/ISIC2017/Visualization/M/TP/ISIC_0014369-eps-converted-to.pdf){width="7.50000%"} ![image](Results/ISIC2017/Visualization/M/FP/ISIC_0015468-eps-converted-to.pdf){width="7.50000%"} ![image](Results/ISIC2017/Visualization/M/FN/ISIC_0013696-eps-converted-to.pdf){width="7.50000%"} ![image](Results/ISIC2017/Visualization/M/TN/ISIC_0016069-eps-converted-to.pdf){width="7.50000%"} ![image](Results/ISIC2017/Visualization/SK/TP/ISIC_0013271-eps-converted-to.pdf){width="7.50000%"} ![image](Results/ISIC2017/Visualization/SK/FP/ISIC_0012484-eps-converted-to.pdf){width="7.50000%"} ![image](Results/ISIC2017/Visualization/SK/FN/ISIC_0012522-eps-converted-to.pdf){width="7.50000%"} ![image](Results/ISIC2017/Visualization/SK/TN/ISIC_0012722-eps-converted-to.pdf){width="7.50000%"}

For the task of lesion classification, we utilized a ResNet-50 [@resnet] architecture pre-trained on ImageNet data set with final fully-connected layer modified to output probabilities of lesion being in each of the classes. We furthermore processed all the images in our training data to remove occlusions as described in the previous section. The pre-processed images were then added to the training data set of the lesion classification model to make it more robust to the presence of occlusions and prevent over-fitting. We then also augmented the data as described before. See Figure \[fig:il\] for the illustration of the process of obtaining training data for the classification network (this is discussed in details in previous sections - they also provide the numerical description of the changes of the data size that are incurred by purification and augmentation steps).

Experimental results
--------------------

![**(Top)** ROC curves obtained by traditional baseline (ResNet-50 pretrained on ImageNet and fine tuned on raw data without any purification/augmentation) **(Left)** and proposed classification model **(Right)** for ISIC 2017 test data-set. **(Bottom)** Confusion matrix obtained by traditional baseline (**left**) and proposed model (**right**). M - melanoma, N - nevus, SK - seborrheic keratosis.[]{data-label="fig:curves"}](Results/class-eps-converted-to.pdf "fig:"){width="23.00000%"} ![**(Top)** ROC curves obtained by traditional baseline (ResNet-50 pretrained on ImageNet and fine tuned on raw data without any purification/augmentation) **(Left)** and proposed classification model **(Right)** for ISIC 2017 test data-set. **(Bottom)** Confusion matrix obtained by traditional baseline (**left**) and proposed model (**right**). M - melanoma, N - nevus, SK - seborrheic keratosis.[]{data-label="fig:curves"}](Results/avgs-eps-converted-to.pdf "fig:"){width="23.00000%"} ![**(Top)** ROC curves obtained by traditional baseline (ResNet-50 pretrained on ImageNet and fine tuned on raw data without any purification/augmentation) **(Left)** and proposed classification model **(Right)** for ISIC 2017 test data-set. **(Bottom)** Confusion matrix obtained by traditional baseline (**left**) and proposed model (**right**). M - melanoma, N - nevus, SK - seborrheic keratosis.[]{data-label="fig:curves"}](Results/conf_base-eps-converted-to.pdf "fig:"){width="23.00000%"} ![**(Top)** ROC curves obtained by traditional baseline (ResNet-50 pretrained on ImageNet and fine tuned on raw data without any purification/augmentation) **(Left)** and proposed classification model **(Right)** for ISIC 2017 test data-set. **(Bottom)** Confusion matrix obtained by traditional baseline (**left**) and proposed model (**right**). M - melanoma, N - nevus, SK - seborrheic keratosis.[]{data-label="fig:curves"}](Results/conf_top-eps-converted-to.pdf "fig:"){width="23.00000%"}

For empirical evaluation we used the images from the ISIC test data set. It contains $600$ images ($117$ melanoma images, $90$ seborrheic keratosis images, and $393$ nevus images), from which we did not remove the hairs and rulers. We used the main evaluation metrics defined in the ISIC 2017 challenge: area under the receiver operating characteristic curve (ROC AUC) for melanoma classification and ROC AUC for melanoma and seborrheic keratosis classifications combined (mean value). We compare our performance with the baseline model, i.e. ResNet-50 pretrained on ImageNet and fine tuned on raw data without performing any purification/augmentation, and the winning models of the ISIC 2017 challenge (note that these models were also using data sets from different digital lesion libraries for training their models, similarly to our approach).

The ROC-AUC values obtained for different models are highlighted in Table \[tab:my\_label\]. We obtained ROC AUC of $0.88$ for melanoma classification and the mean performance of $0.915$ and outperform both the baseline, as shown in Figure \[fig:curves\], as well as the winners of the challenge that obtained ROC AUC of $0.874$ for melanoma and average ROC AUC of $0.911$. Thus, leading to performance improvement of order $\approx 4\%$ We also report there the resulting specificity values for different values of sensitivity for melanoma classification. Note that in a separate study by Esteva et al. [@Esteva2017], dermatologists were asked to classify dermoscopic images into three categories: benign, malignant (melanoma) and non-neoplastic. Two dermatologists attained $65.56\%$ and $66.0\%$ accuracy on a small subset of the data corpus owned and maintained by the research group and not open-sourced to public. We obtain $81.6\%$ accuracy on the task of classifying images into three categories (this number was computed based on the confusion matrix in Figure \[fig:curves\]): benign (nevus), malignant (melanoma), and seborrheic keratosis which, in the prism of this study, is much higher than the average accuracy achieved by dermatologists.

  -------------------------------------------------------------------------------------------------- ----------- -----------
                                                                                                                 
                                                Model                                                 Melanoma       Avg
                                         Traditional Baseline                                           0.805       0.873
                                            Proposed Model                                            **0.880**   **0.915**
   ISIC Challenge winners [@DBLP:journals/corr/MatsunagaHMK17; @DBLP:journals/corr/MenegolaTFLAV17]     0.874       0.911
  -------------------------------------------------------------------------------------------------- ----------- -----------

  : ROC AUC values for traditional baseline (ResNet-50 pretrained on ImageNet and fine-tuned of raw data without any purification/augmentation) and winners for ISIC challenge.[]{data-label="tab:my_label"}

                      Method                       $82\%$      $89\%$      $95\%$
  ---------------------------------------------- ----------- ----------- -----------
   Top AVG [@DBLP:journals/corr/MatsunagaHMK17]     0.729       0.588       0.366
   Top SK [@DBLP:journals/corr/Gonzalez-Diaz17]     0.727       0.555       0.404
   Top M [@DBLP:journals/corr/MenegolaTFLAV17]    **0.747**     0.590       0.395
             Our Classification Model               0.697     **0.648**   **0.492**

  : Specificity values at sensitivity levels of $82\%/89\%/95\%$ for melanoma classification. Top AVG, Top SK, and Top M denote the winning approaches of the ISIC 2017 challenge.[]{data-label="tab:values1"}

In Figure \[fig:curves\] we report the confusion matrix for predicting melanoma, seborrheic keratosis, and nevus. In Table \[tab:values1\] we also report the resulting specificity values for different values of sensitivity for melanoma classification. It is clearly observed that we show superior performance over other methods for different levels of sensitivity.

The confusion matrix in Figure \[fig:curves\] shows that the classification model occasionally confuses nevus and melanoma images. We employ visualization technique called VisualBackProp to discover the reasons that led to these false positives and false negatives. The false positive case in (Figure \[fig:class\_visualization\], Nevus) indicates that the lesion area is not well-identified by the deep learning model (most likely because it is blended with the skin surface). The false negative cases show that the occlusions still sometimes affect the prediction, though the pre-processing step has eliminated many such similar mis-classification cases, .: see true positive cases for nevus (the model handles well hair occlusions) and seborrheic keratosis (the model leaves out the ink and focuses on the lesion).

Conclusion {#sec:CC}
==========

The techniques that aim at automating the visual examination of skin lesions, traditionally done by dermatologists, are nowadays dominated by the deep-learning-based methods. These methods are the most accurate and scalable, but they require large training data sets and thus their applicability in dermatology is compromised by the size of the publicly available dermatological data sets, which are often small and contain occlusions. We present a solution for this problem that relies on careful data purification that removes common occlusions from dermoscopic images and augmentation that uses the modern technique of deep-learning-based data generation to improve data balancedness. We demonstrate the effectiveness of our system on the lesion classification task.

[^1]: Open-sourced under the link <https://github.com/devansh20la/Beating-Melanoma>
