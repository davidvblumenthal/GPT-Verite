{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidblumenthal/opt/miniconda3/envs/trans/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wikipediaapi\n",
    "import jsonlines\n",
    "import spacy\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path: str) -> list:\n",
    "    samples = []\n",
    "    with jsonlines.open(path) as input:\n",
    "        for line in input:\n",
    "            samples.append(line)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(path, liste):\n",
    "    \n",
    "    with jsonlines.open(path, \"w\") as writer:\n",
    "        for sample in liste:\n",
    "            writer.write(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORTANT_ENT_TYPE = set(\n",
    "    [\"ORG\", \"GPE\", \"PERSON\", \"WORK_OF_ART\", \"PRODUCT\", \"EVENT\"]\n",
    ")  # added GPE\n",
    "REMOVE_ENT_TYPE = set([\"ORDINAL\", \"CARDINAL\"])\n",
    "\n",
    "def obtain_important_ne(gen, include_capitalized_words_as_ents=True):\n",
    "    important_words = []\n",
    "\n",
    "    doc = nlp(gen)\n",
    "\n",
    "    # print(\"GEN: \", gen)\n",
    "    # print([(token.text, token.pos_, token.tag_, token.dep_) for token in doc if token.pos_ in ['NOUN', 'PRON', 'PROPN']])\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    ents = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    if include_capitalized_words_as_ents and len(ents) == 0:\n",
    "        capitalized_words = re.findall(\"(?<!^)([A-Z][a-z]+)\", gen)\n",
    "\n",
    "        if len(capitalized_words) > 0:\n",
    "            capitalized_words = [\n",
    "                (word, \"CAPITALIZED\")\n",
    "                for word in capitalized_words\n",
    "                if word.lower() not in stop_words\n",
    "            ]\n",
    "            ents.extend(capitalized_words)\n",
    "\n",
    "    important_words.extend([ent for ent in ents if ent[1] in IMPORTANT_ENT_TYPE])\n",
    "    remaining_ne_all = [ent for ent in ents if ent[1] not in IMPORTANT_ENT_TYPE]\n",
    "\n",
    "    # filter out some ne\n",
    "    remaining_ne = []\n",
    "    for ent in remaining_ne_all:\n",
    "        if ent[1] in REMOVE_ENT_TYPE:\n",
    "            continue\n",
    "        if ent[1] == \"DATE\" and (\n",
    "            \"year\" in ent[0] or \"day\" in ent[0]\n",
    "        ):  # not bool(re.search(r'\\d', ent[0])):\n",
    "            # if \"DATE\" entity contains NO number at all (e.g., ``the year''), meaningless\n",
    "            continue\n",
    "        remaining_ne.append(ent)\n",
    "\n",
    "    gens_with_ne = {\n",
    "        \"gen\": gen,\n",
    "        \"important_ne\": important_words,\n",
    "        \"unimportant_ne\": remaining_ne,\n",
    "        \"subject\": set(\n",
    "            [token.text for token in doc if token.dep_ in [\"nsubj\", \"nsubjpass\"]]\n",
    "        ),\n",
    "        # \"all_analysis\": [(token.text, token.pos_, token.tag_, token.dep_) for token in doc]\n",
    "    }\n",
    "\n",
    "    return gens_with_ne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_fact = read_jsonl(\"/Users/davidblumenthal/Documents/Master_Thesis/Evaluation/Improve_FactualityPrompt/FactualityPrompt/prompts/fever_factual_final.jsonl\")\n",
    "prompts_non_fact = read_jsonl(\"/Users/davidblumenthal/Documents/Master_Thesis/Evaluation/Improve_FactualityPrompt/FactualityPrompt/prompts/fever_nonfactual_final.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_factual = []\n",
    "ent_non_factual = []\n",
    "\n",
    "for sample_f, sample_nf in zip(prompts_fact, prompts_non_fact):\n",
    "    # [[\"The Host (2013 film)\", \"34465253\"], [\"Saoirse Ronan\", \"11061022\"]]\n",
    "    belong_together = []\n",
    "    for ent in sample_f[\"evidence_info\"]:\n",
    "        belong_together.append(ent[0])\n",
    "    ent_factual.append(belong_together)\n",
    "\n",
    "    belong_together = []\n",
    "    for ent in sample_nf[\"evidence_info\"]:\n",
    "         belong_together.append(ent[0])   \n",
    "    ent_non_factual.append(belong_together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(entity_list: list) -> list:\n",
    "    overall_content = []\n",
    "    for sample in entity_list:\n",
    "        content_sample = []\n",
    "        for entity in sample:\n",
    "            page = wiki_wiki.page(entity)\n",
    "            content = page.text\n",
    "\n",
    "            content_sample.append({\"title\": entity, \"text\": content})\n",
    "        \n",
    "        overall_content.append({\"entity_prompt\": content_sample[0][\"title\"], \"articles\": content_sample})\n",
    "\n",
    "    return overall_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting articles from Wiki -> factual\n",
    "factual_contents = extract_content(ent_factual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting articles from Wiki -> non_factual\n",
    "non_factual_contents = extract_content(ent_non_factual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(path=\"./factual_wiki_crawl.jsonl\", liste=factual_contents)\n",
    "write_jsonl(path=\"./nonfactual_wiki_crawl.jsonl\", liste=non_factual_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(len(factual_contents))\n",
    "print(len(non_factual_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, article in enumerate(contents):\n",
    "\n",
    "    obj = obtain_important_ne(article[\"text\"])\n",
    "\n",
    "    nes = [ne[0] for ne in obj[\"important_ne\"]]\n",
    "\n",
    "    nes = list(set(nes))\n",
    "\n",
    "    contents[idx] = {\"title\": article[\"title\"], \"important_ne\": nes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nes(samples: list, just_first=False):\n",
    "    \n",
    "    samples_copy = deepcopy(samples)\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "        {\"entity_prompt\": content_sample[0][\"title\"], \"articles\": [{\"title\": entity, \"text\": content}]})\n",
    "    \"\"\"\n",
    "    for sample in tqdm(samples_copy):\n",
    "        all_nes = []\n",
    "        for article in sample[\"articles\"]:\n",
    "            \n",
    "            text = article[\"text\"]\n",
    "            text = text.strip()\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "            ne_obj = obtain_important_ne(text)\n",
    "            nes = [ne[0] for ne in ne_obj[\"important_ne\"]]\n",
    "\n",
    "            all_nes.extend(nes)\n",
    "            \n",
    "        all_nes = list(set(all_nes))\n",
    "\n",
    "        sample[\"important_ne\"] = all_nes\n",
    "\n",
    "    return samples_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [2:33:06<00:00,  1.15s/it]  \n"
     ]
    }
   ],
   "source": [
    "add_nes_fact = extract_nes(factual_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(path=\"./factual_wiki_crawl_imp_ne.jsonl\", liste=add_nes_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [2:08:58<00:00,  1.03it/s]  \n"
     ]
    }
   ],
   "source": [
    "non_factual_contents = read_jsonl(\"./nonfactual_wiki_crawl.jsonl\")\n",
    "add_nes_nonfact = extract_nes(non_factual_contents)\n",
    "write_jsonl(path=\"./nonfactual_wiki_crawl_imp_ne.jsonl\", liste=add_nes_nonfact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88e66a280c6b5a9265e687112bcf2cdbea5d603c570f9ada8ea8c1b275262856"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
